\chapter{Results: methods comparison}

\textbf{colocar resultados y plots de comparacion}

In this chapter we will study different data sets using the two presented methods for the \textit{BGSEES} algorithm: the \textbf{Least Squares} and \textbf{Decrease Range} method. 

The aim of the chapter is to test them against each other and using different parameters to see which method yields the best result

The algorithm is tested using data from both Solar flares and flares from far-away stars. Because the day hemisphere has to be discarded in order to study far-away stars (because of the Sun's effect on the ionosphere), the first section will study examples of flares originating from the Sun and the second those from outside the Solar System. -> En este capitulo al final solo sol, estrellas en el siguiente

For the solar flares, the ti files of days when a flare had taken place were used to compare the results. The data was filtered around the time of the flare (30 minutes before and after, if there was an exact moment in time).
 
Below is the list of times (\textit{year.day.seconds}) of the different flares we studied. The seconds are either an exact moment in time or a range used in the plots of the papers the flares are listed in.

Flares listed in \textit{"GNSS measurement of EUV photons flux rate during strong and mid solar flares"}\cite{hernandez2012gnss}

LA LISTA HA CAMBIADO

\begin{itemize}
	\item 2003.301.39777
	\item 2003.308.71000-71100
	\item 2005.020.24200-24400
	\item 2006.340.67300-67500
	\item 2011.210.44134
	\item 2011.216.13908
\end{itemize}

And those listed in \textit{"GPS as a solar observational instrument: Real-time estimation of EUV photons flux rate during strong, medium, and weak solar flares"}\cite{singh2015gps}

\begin{itemize}
	\item 2001.334.3700-4000
	\item 2001.347.51800-52200
	\item 2002.196.72240
	\item 2004.204.27800-28000
	\item 2004.310.41370
	\item 2004.313.52500
	\item 2005.258.30990
	\item 2012.066.4400-4700
	\item 2012.130.50600-51000
	\item 2012.297.11600-12000
	\item 2013.310.35970
\end{itemize}

To perform this study, the best epoch within the given range is found using the mean VTEC, as shown in chapter 5. The data is then filtered using this epoch and the algorithm is executed using the necessary parameters, the studied factors are:

The \textbf{execution time} of the algorithm and the \textbf{absolute error} of the estimation, obtained by computing the angle between correct Sun position\footnote{The correct Sun position at that moment is obtained from the data, it is one of the many fields the ti files contain} and the estimated location, using the same operations we've used in previous chapters to compute the angle. 

The ti files that contain data for the entire day are filtered using this bash script, which has a list with the information of each file to be filtered: the name of the original file and the upper and lower limits of time (or a specific moment used to compute the limits). It then filters each file using a simple AWK one-line script that checks the field with the time:

\begin{minipage}{\linewidth}
	\begin{lstlisting}[language=Bash, caption=Filtering the ti file]
#!/bin/bash	
	
strings=(
	'2003.301,36000,41400'
	'2011.210,44134'
	[..All the filenames..] 
	'2012.130,50600,51000'
	'2012.297,11600,12000'
)

tiDataFolder="/home/mbdavid2/Documents/dataTi/"

for i in "${strings[@]}"; do
	dataInfo="$i"
	
	# Split the information
	arrayInfo=(${dataInfo//,/ })
	
	# Use the range if specified, compute it otherwise
	if [ ${#arrayInfo[@]} = 2 ]; then
		let lowerLimit="${arrayInfo[1]}"-1800
		let upperLimit="${arrayInfo[1]}"+1800
	else
		let lowerLimit="${arrayInfo[1]}"
		let upperLimit="${arrayInfo[2]}"
	fi
	
	# Name the file according to the parameters
	tiDataFile="ti.""${arrayInfo[0]}"
	outputFileName="$tiDataFile"".""$lowerLimit""-""$upperLimit"
	echo $outputFileName
	
	# Filter and compress
	zcat "$tiDataFolder""originals/""$tiDataFile" 
	| gawk -v lower="$lowerLimit" -v upper="$upperLimit" 
	'{/a/; if ($3 >= lower/3600 && $3 <= upper/3600) {print $0;}}' 
	> "$tiDataFolder""$outputFileName"
	gzip -f "$tiDataFolder""$outputFileName" # -f to force overwrite
done\end{lstlisting}
\end{minipage}

The study is divided in three categories, based on the method used to discard outliers from the input data:

\begin{itemize}
	\item Using the data from \textbf{all IPPs} without filtering out any outliers
	\item Using a \textbf{cutoff value} for the VTEC only when \textbf{finding the spike}
	\item Using a \textbf{cutoff value} for \textbf{all the VTEC data} that will be used for the computations of the algorithm (si ???)
	\item Using \textbf{linear fit} for the Decreasing Range method to discard outliers and \textbf{multiple iterations} for the Least Squares method to try to improve the solution, both explained in their respective chapters.
\end{itemize}

\section{Using all available data}

The main problem of this method is that outliers that are far from the sample make the mean computation unstable, which causes the algorithm to use incorrect epochs. Furthermore, the outliers can cause numerical instability in some of the methods' computations.

\subsection{Decreasing range}

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Total estimation error (degrees) & Time (seconds) \\ [0.5ex] 
		\hline\hline
		ti.2001.347.gz & 113.813 & 1.03879 \\
		\hline
		ti.2002.196.gz & 83.5147 & 0.26934 \\
		\hline
		ti.2003.301.gz & 24.6405 & 1.07813 \\
		\hline
		ti.2003.308.gz & 128.59 & 0.971644 \\
		\hline
		ti.2005.020.gz & 20.1031 & 0.916863 \\
		\hline
		ti.2005.258.gz & 91.3298 & 0.498707 \\
		\hline
		ti.2011.210.gz & 90.5716 & 1.46812 \\
		\hline
		ti.2012.066.gz & 133.236 & 2.30571 \\
		\hline
		ti.2012.130.gz & 162.888 & 2.49292 \\
		\hline
		ti.2012.297.gz & 78.487 & 0.789705 \\
		\hline\hline
		Total & 927.174 & 11.8299 \\
		\hline
	\end{tabular}
	\caption{Results for different data sets}
\end{table}

\subsection{Least Squares}

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Total estimation error (degrees) & Time (seconds) \\ [0.5ex] 
		\hline\hline
		ti.2001.347.gz & 106.064 & 0.0175844 \\
		\hline
		ti.2002.196.gz & 66.2043 & 0.0158694 \\
		\hline
		ti.2003.301.gz & 42.2689 & 0.0154119 \\
		\hline
		ti.2003.308.gz & 55.4949 & 0.326298 \\
		\hline
		ti.2005.020.gz & 124.218 & 0.14858 \\
		\hline
		ti.2005.258.gz & 111.073 & 0.0264138 \\
		\hline
		ti.2011.210.gz & 98.776 & 0.0558694 \\
		\hline
		ti.2012.066.gz & 64.186 & 0.0143492 \\
		\hline
		ti.2012.130.gz & 73.1871 & 0.0321778 \\
		\hline
		ti.2012.297.gz & 47.0189 & 0.00680915 \\
		\hline\hline
		Total & 788.491 & 0.659363 \\
		\hline
	\end{tabular}
	\caption{Results for different data sets}
\end{table}

\subsection{Discussion}

\section{Direct VTEC filter}

This is the first method used in the brute force approach that yielded results very similar to the real location of the Sun, but it does not work well with other data sets.

The filter is done using a cutoff value for the VTEC when the data is first filtered

Different cutoff values are studied to see if this is a reliable method, as it simplifies the overall computation of the algorithm considerably.


\begin{table}[h!]
   	\centering
   	\def\arraystretch{1.2}
   	\begin{tabular}{|c c c|} 
   		\hline
   		Data set & Total estimation error (degrees) & Time (seconds) \\ [0.5ex] 
   		\hline\hline
   		ti.2001.347.gz & 3.53947 & 1.03243 \\
   		\hline
   		ti.2002.196.gz & 27.6877 & 0.287287 \\
   		\hline
   		ti.2003.301.gz & 3.93239 & 1.36538 \\
   		\hline
   		ti.2003.308.gz & 131.366 & 0.891328 \\
   		\hline
   		ti.2005.020.gz & 64.8737 & 0.551884 \\
   		\hline
   		ti.2005.258.gz & 48.7806 & 1.00214 \\
   		\hline
   		ti.2011.210.gz & 126.204 & 1.23266 \\
   		\hline
   		ti.2012.066.gz & 75.1081 & 1.85402 \\
   		\hline
   		ti.2012.130.gz & 56.7937 & 2.42187 \\
   		\hline
   		ti.2012.297.gz & 1.46042 & 2.86145 \\
   		\hline
   		\hline
   		Total & 539.746 & 13.5005 \\
   		\hline
   	\end{tabular}
   	\caption{Results for different data sets}
\end{table}

\subsection{Least Squares}

\begin{table}[h!]
   	\centering
   	\def\arraystretch{1.2}
   	\begin{tabular}{|c c c|} 
   		\hline
   		Data set & Total estimation error (degrees) & Time (seconds) \\ [0.5ex] 
   		\hline\hline
   		ti.2001.347.gz & 3.41832 & 0.0114944 \\
   		\hline
   		ti.2002.196.gz & 46.578 & 0.00492704 \\
   		\hline
   		ti.2003.301.gz & 4.57509 & 0.00724225 \\
   		\hline
   		ti.2003.308.gz & 141.865 & 0.378145 \\
   		\hline
   		ti.2005.020.gz & 38.3263 & 0.129509 \\
   		\hline
   		ti.2005.258.gz & 1.88011 & 0.0103007 \\
   		\hline
   		ti.2011.210.gz & 38.5213 & 0.0603164 \\
   		\hline
   		ti.2012.066.gz & 70.1063 & 0.0251154 \\
   		\hline
   		ti.2012.130.gz & 9.26238 & 0.0518934 \\
   		\hline
   		ti.2012.297.gz & 3.00704 & 0.0392889 \\
   		\hline\hline
   		Total & 357.54 & 0.718233 \\
   		\hline
   	\end{tabular}
   	\caption{Results for different data sets}\label{tab:lsDirect}
\end{table}

\subsection{Discussion}

As we can see

Considering how for both cases using the direct filter yields significantly better results, the next sections use this for filtering the first traversal of the data (when find the epoch and filtering by time).

\section{Decreasing range: linear fit}

explain: best combination 3 y 4

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Total estimation error (degrees) & Time (seconds) \\ [0.5ex] 
		\hline\hline
		ti.2001.347.gz & 3.66657 & 45.4151 \\
		\hline
		ti.2002.196.gz & 30.9237 & 42.6182 \\
		\hline
		ti.2003.301.gz & 3.57674 & 51.7222 \\
		\hline
		ti.2003.308.gz & 79.3679 & 43.6955 \\
		\hline
		ti.2005.020.gz & 68.911 & 37.3023 \\
		\hline
		ti.2005.258.gz & 22.6375 & 50.5795 \\
		\hline
		ti.2011.210.gz & 19.7344 & 55.8291 \\
		\hline
		ti.2012.066.gz & 84.3076 & 31.9302 \\
		\hline
		ti.2012.130.gz & 21.896 & 42.326 \\
		\hline
		ti.2012.297.gz & 0.679492 & 53.7369 \\
		\hline
		Total & 335.701 & 455.155 \\
		\hline
	\end{tabular}
	\caption{Results for different data sets}
\end{table}

\section{Least Squares: Iterations}

10 iterations, saves the result of the best iteration.

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Total estimation error (degrees) & Time (seconds) \\ [0.5ex] 
		\hline\hline
		ti.2001.347.gz & 3.41832 & 0.0194832 \\
		\hline
		ti.2002.196.gz & 46.578 & 0.00818968 \\
		\hline
		ti.2003.301.gz & 4.57509 & 0.0171783 \\
		\hline
		ti.2003.308.gz & 141.865 & 0.366494 \\
		\hline
		ti.2005.020.gz & 38.3263 & 0.141508 \\
		\hline
		ti.2005.258.gz & 1.88011 & 0.0177967 \\
		\hline
		ti.2011.210.gz & 38.5213 & 0.0704044 \\
		\hline
		ti.2012.066.gz & 70.1063 & 0.049831 \\
		\hline
		ti.2012.130.gz & 9.26238 & 0.0711422 \\
		\hline
		ti.2012.297.gz & 3.00704 & 0.0592826 \\
		\hline
		Total & 357.54 & 0.82131 \\
		\hline
	\end{tabular}
	\caption{Results for different data sets}
\end{table}

As we can see, the results in terms of estimation error are exactly the same as the ones seen in table \ref{tab:lsDirect} with a slight increase in execution time due to the number of iterations. Because there is no improvement with consecutive iterations, the algorithm just keeps the best result from the first one, hence the same results.

\section{Least Squares estimation error}

As we have seen in the previous chapter, using the estimation error to find the best estimation does not work because.... But it can be intersting to see how the estimation error evolves compared to the real error for all the studied data sets, the following plot shows:

\section{Discussion}












\clearpage
\clearpage
\clearpage
\clearpage









\section{Far-away stars}

\subsection{Used data sets}










\section{Sun}

the two data sets for the best epoch

the first one, study all epochs and see what happens even when there's no flare

\subsection{Decrease range method}

Instead of focusing on only the best epoch\footnote{The best epoch is the one with the highest mean VTEC of all its IPPs}, the following table presents the 15 best epochs from best to worst (with 11.05 at the top, the one studied in previous chapters) and their results: 

\begin{itemize}
	\item The right ascension and declination of the estimated source location (the Sun in this case)
	\item The error of the previous results compared to the real Sun's location
	\item The Pearson correlation coefficient of the estimated Sun
\end{itemize}

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c c c c|} 
		\hline
		\textbf{Epoch} & RA & Dec & RA Error & Dec Error & Correlation Coefficient \\ [0.5ex] 
		\hline\hline
		11.05 & 213.75 & -10.3125 & 1.412 & 2.7475 & 0.949659 \\ 
		\hline
		11.075 & 215.625 & -12.1875 & 3.287 & 0.8725 & 0.910935 \\ 
		\hline
		11.0417 & 210.938 & -9.375 & 1.4005 & 3.685 & 0.941128 \\ 
		\hline
		11.1167 & 211.875 & -19.6875 & 0.463 & 6.6275 & 0.669265 \\ 
		\hline
		11.0333 & 225.938 & -18.75 & 13.5995 & 5.69 & 0.564307 \\ 
		\hline
		11.025 & 219.375 & -16.875 & 7.037 & 3.815 & 0.49188 \\ 
		\hline
		11.0917 & 216.562 & -21.5625 & 4.2245 & 8.5025 & 0.48441 \\ 
		\hline
		11.1333 & 214.688 & -23.4375 & 2.3495 & 10.3775 & 0.397166 \\ 
		\hline
		11.1917 & 211.875 & -11.25 & 0.463 & 1.81 & 0.355636 \\ 
		\hline
		11.3667 & 185.625 & -76.875 & 26.713 & 63.815 & 0.23138 \\ 
		\hline
		10.8583 & 20.625 & -65.625 & 191.713 & 52.565 & 0.112926 \\ 
		\hline
		11.1583 & 211.875 & -32.8125 & 0.463 & 19.7525 & 0.29224 \\ 
		\hline
		10.9917 & 62.8125 & -71.25 & 149.525 & 58.19 & 0.130575 \\ 
		\hline
		11.0167 & 203.438 & 8.4375 & 8.9005 & 21.4975 & 0.211087 \\ 
		\hline
		11.4583 & 235.312 & -37.5 & 22.9745 & 24.44 & 0.137234 \\
		\hline
	\end{tabular}
	\caption{Results for different epochs}
\end{table}




\section{Far-away stars}


