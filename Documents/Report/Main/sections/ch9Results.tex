\chapter{Results: solar flares}

In this chapter different data sets will be studied using the two presented methods for the \textit{BGSEES} algorithm: the \textbf{Least Squares} and \textbf{Decrease Range} methods. 

The aim of the chapter is to test them against each other and using different parameters to see which method yields the best results.

The algorithm is tested using data from Solar flares, for these, the ti files of days when a flare had taken place were used to compare the results. The data was filtered around the time of the flare (30 minutes before and after, if there was an exact moment in time).
 
Below is the list of times (\textit{year.day.seconds}) of the different flares we studied. The seconds are either an exact moment in time or a range used in the plots of the papers the flares are listed in.

These are the flares listed in the paper \textit{"GNSS measurement of EUV photons flux rate during strong and mid solar flares"}\cite{hernandez2012gnss}

\begin{itemize}
	\item 2003.301.39777
	\item 2003.308.71000-71100
	\item 2005.020.24200-24400
%	\item 2006.340.67300-67500
	\item 2011.210.44134
%	\item 2011.216.13908
\end{itemize}

And those listed in \textit{"GPS as a solar observational instrument: Real-time estimation of EUV photons flux rate during strong, medium, and weak solar flares"}\cite{singh2015gps}

\begin{itemize}
%	\item 2001.334.3700-4000
	\item 2001.347.51800-52200
	\item 2002.196.72240
%	\item 2004.204.27800-28000
%	\item 2004.310.41370
%	\item 2004.313.52500
	\item 2005.258.30990
	\item 2012.066.4400-4700
	\item 2012.130.50600-51000
	\item 2012.297.11600-12000
%	\item 2013.310.35970
\end{itemize}

To perform this study, the best epoch within the given range is found using the mean VTEC, as shown in chapter 5. The data is then filtered using this epoch and the algorithm is executed using the necessary parameters, outputing the \textbf{execution time} of the algorithm and the \textbf{absolute error} of the estimation, obtained by computing the angle between correct Sun position\footnote{The correct Sun position at that moment is obtained from the data, it is one of the many fields the ti files contain} and the estimated location, using the same operations we have used in previous chapters to compute the angle. 

\begin{minipage}{\linewidth}
	\begin{lstlisting}[language=Bash, caption=Filtering the ti file]
#!/bin/bash	
strings=(
	'2003.301,36000,41400'
	'2011.210,44134'
	[..All the filenames..] 
	'2012.297,11600,12000'
)
tiDataFolder="/home/mbdavid2/Documents/dataTi/"
for i in "${strings[@]}"; do
	dataInfo="$i"
	
	# Split the information
	arrayInfo=(${dataInfo//,/ })
	
	# Use the range if specified, compute it otherwise
	if [ ${#arrayInfo[@]} = 2 ]; then
		let lowerLimit="${arrayInfo[1]}"-1800
		let upperLimit="${arrayInfo[1]}"+1800
	else
		let lowerLimit="${arrayInfo[1]}"
		let upperLimit="${arrayInfo[2]}"
	fi
	
	# Name the file according to the parameters
	tiDataFile="ti.""${arrayInfo[0]}"
	outputFileName="$tiDataFile"".""$lowerLimit""-""$upperLimit"
	
	# Filter and compress
	zcat "$tiDataFolder""originals/""$tiDataFile" 
	| gawk -v lower="$lowerLimit" -v upper="$upperLimit" 
	'{/a/; if ($3 >= lower/3600 && $3 <= upper/3600) {print $0;}}' 
	> "$tiDataFolder""$outputFileName"
	gzip -f "$tiDataFolder""$outputFileName" # -f to force overwrite
done\end{lstlisting}
\end{minipage}

The ti files that contain data for the entire day are filtered using the previous bash script, which has a list with the information of each file to be filtered: the name of the original file and the upper and lower limits of time (or a specific moment used to compute the limits). It then filters each file using a simple AWK one-line script that checks the field with the time.

The study is divided in three categories, based on the method used to discard outliers from the input data:

\begin{itemize}
	\item Using the data from \textbf{all IPPs} without filtering out any outliers
	\item Using a \textbf{cutoff value} for \textbf{all the VTEC data} that will be used for the computations of the algorithm
	\item Using \textbf{linear fit} for the Decreasing Range method to discard outliers and \textbf{multiple iterations} for the Least Squares method to try to improve the solution, both presented in their respective chapters.
	\item Using data from \textbf{multiple epochs} rather than just one.
\end{itemize}

\clearpage

\section{Using all available data}

For this first test, all the data from the ti file is used: in the case of the Decreasing Range (DR) method, all the VTEC values that could be outliers are used to compute the correlation, and in the case of the Least Squares (LS) method, all are used for the equations of the system.

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.3ex] 
		\hline\hline
		2001.347 & 113.813 & 1.03879 \\
		\hline
		2002.196 & 83.5147 & 0.26934 \\
		\hline
		2003.301 & 24.6405 & 1.07813 \\
		\hline
		2003.308 & 128.59 & 0.971644 \\
		\hline
		2005.020 & 20.1031 & 0.916863 \\
		\hline
		2005.258 & 91.3298 & 0.498707 \\
		\hline
		2011.210 & 90.5716 & 1.46812 \\
		\hline
		2012.066 & 133.236 & 2.30571 \\
		\hline
		2012.130 & 162.888 & 2.49292 \\
		\hline
		2012.297 & 78.487 & 0.789705 \\
		\hline
		Total & 927.174 & 11.8299 \\
		\hline
	\end{tabular}
	\quad\quad\quad
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.3ex] 
		\hline\hline
		2001.347 & 106.064 & 0.0175844 \\
		\hline
		2002.196 & 66.2043 & 0.0158694 \\
		\hline
		2003.301 & 42.2689 & 0.0154119 \\
		\hline
		2003.308 & 55.4949 & 0.326298 \\
		\hline
		2005.020 & 124.218 & 0.14858 \\
		\hline
		2005.258 & 111.073 & 0.0264138 \\
		\hline
		2011.210 & 98.776 & 0.0558694 \\
		\hline
		2012.066 & 64.186 & 0.0143492 \\
		\hline
		2012.130 & 73.1871 & 0.0321778 \\
		\hline
		2012.297 & 47.0189 & 0.00680915 \\
		\hline\hline
		Total & 788.491 & 0.659363 \\
		\hline
	\end{tabular}
	\caption{Estimation error and execution time for different data sets using the DR (left) and LS (right) methods without any filter}
\end{table}

The main problem of this method is that outliers can cause the computation of the mean to be unstable, which causes the algorithm to use incorrect epochs.

Furthermore, the outliers can cause numerical instability in some of the methods' computations, if one has a large value, for example, the computation of the correlation relies on the sum of the square of the VTEC value, so the total can lead to incorrect results because of the rapid increase of this variable.

Additionally, the LS method is significantly faster: all data sets together add up to a total execution time of less than one second, whilst the DR method needs that time or even more for almost each of the data sets.

While in all previous sections a direct VTEC filter has been used, using all data was tested too to compare the results.

\clearpage

\section{Direct VTEC filter}

Although this is a very simple approach to discard outliers, we decided to test it because the value of the Delta VTEC does not usually surpass values such as 0.7, only some IPPs might present values like this, due to other interferences in the satellite data. The flare from the day 301 of 2003, for example, studied in previous chapters, is one of the most powerful flares ever recorded, and the peak value of the Delta VTEC is 0.4.

These are the results of the execution using a cutoff value of 0.7. This value was selected as it is the one that yielded the best results in a range of 0.3 to 1:

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.3ex] 
		\hline\hline
		2001.347 & 3.53947 & 1.03243 \\
		\hline
		2002.196 & 27.6877 & 0.287287 \\
		\hline
		2003.301 & 3.93239 & 1.36538 \\
		\hline
		2003.308 & 131.366 & 0.891328 \\
		\hline
		2005.020 & 64.8737 & 0.551884 \\
		\hline
		2005.258 & 48.7806 & 1.00214 \\
		\hline
		2011.210 & 126.204 & 1.23266 \\
		\hline
		2012.066 & 75.1081 & 1.85402 \\
		\hline
		2012.130 & 56.7937 & 2.42187 \\
		\hline
		2012.297 & 1.46042 & 2.86145 \\
		\hline
		Total & 539.746 & 13.5005 \\
		\hline
	\end{tabular}
	\quad\quad\quad
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.3ex] 
		\hline\hline
		2001.347 & 3.41832 & 0.0114944 \\
		\hline
		2002.196 & 46.578 & 0.00492704 \\
		\hline
		2003.301 & 4.57509 & 0.00724225 \\
		\hline
		2003.308 & 141.865 & 0.378145 \\
		\hline
		2005.020 & 38.3263 & 0.129509 \\
		\hline
		2005.258 & 1.88011 & 0.0103007 \\
		\hline
		2011.210 & 38.5213 & 0.0603164 \\
		\hline
		2012.066 & 70.1063 & 0.0251154 \\
		\hline
		2012.130 & 9.26238 & 0.0518934 \\
		\hline
		2012.297 & 3.00704 & 0.0392889 \\
		\hline
		Total & 357.54 & 0.718233 \\
		\hline
	\end{tabular}
	\caption{Estimation error and execution time for different data sets using the DR (left) and LS (right) methods with a cutoff filter}
	\label{tb:basicCutoff}
\end{table}

As we can see, there has been a significant improvement for both methods in some of the flares with low error values (although some still present a high error), and because this filter is applied when performing the filtering by time of the data, it does not have an impact on the complexity of the execution. 

Because the direct filter yields significantly better results for both cases, the next sections use it for filtering the first traversal of the data (when the necessary data is obtained from the ti file), before performing an additional filter to attempt to discard outliers.

\clearpage

\section{Decreasing range: linear fit}

This approach, introduced in chapter \ref{decRangeChapter}, finds the straight line that fits the data set and discards outliers based on a sigma parameter. This filter can be performed several times (the number of iterations). The results of the decreasing range method using a sigma of 3 and 4 iterations (the best combination of those values in a range from 1 to 10) is shown below:

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.5ex] 
		\hline\hline
		2001.347.gz & 3.66657 & 45.4151 \\
		\hline
		2002.196.gz & 30.9237 & 42.6182 \\
		\hline
		2003.301.gz & 3.57674 & 51.7222 \\
		\hline
		2003.308.gz & 79.3679 & 43.6955 \\
		\hline
		2005.020.gz & 68.911 & 37.3023 \\
		\hline
		2005.258.gz & 22.6375 & 50.5795 \\
		\hline
		2011.210.gz & 19.7344 & 55.8291 \\
		\hline
		2012.066.gz & 84.3076 & 31.9302 \\
		\hline
		2012.130.gz & 21.896 & 42.326 \\
		\hline
		2012.297.gz & 0.679492 & 53.7369 \\
		\hline
		Total & 335.701 & 455.155 \\
		\hline
	\end{tabular}
	\caption{Results for different data sets using linear fit with the DR method}
\end{table}

While this approach improves upon the solution of the Least Squares method (from 357.54 to 335.701), it requires a significantly higher execution time: each execution takes almost a minute, which is more than the total execution time for all data sets using the same method in the previous section.

\clearpage

\section{Least Squares: Iterations}

\subsection{Discarding by source position}

In chapter \ref{LSmethodChapter} iterations did not provide better results for a specific case, but we decided to test it using all data sets in case there was any overall improvement. These are the results of executing the algorithm with 10 iterations, saving the result of the best iteration. The best iteration is that which has the least estimation error, given by the covariance matrix (as seen in \ref{sssec:covariance}):

\begin{figure}[!htb]
	\begin{subfigure}[b]{0.5\textwidth}
		\begin{centering}
		\def\arraystretch{1.2}
		\begin{tabular}{|c c c|} 
			\hline
			Data set & Error (º) & Time (s) \\ [0.5ex] 
			\hline\hline
			2001.347 & 3.41832 & 0.0194832 \\
			\hline
			2002.196 & 46.578 & 0.00818968 \\
			\hline
			2003.301 & 4.57509 & 0.0171783 \\
			\hline
			2003.308 & 141.865 & 0.366494 \\
			\hline
			2005.020 & 38.3263 & 0.141508 \\
			\hline
			2005.258 & 1.88011 & 0.0177967 \\
			\hline
			2011.210 & 38.5213 & 0.0704044 \\
			\hline
			2012.066 & 70.1063 & 0.049831 \\
			\hline
			2012.130 & 9.26238 & 0.0711422 \\
			\hline
			2012.297 & 3.00704 & 0.0592826 \\
			\hline
			Total & 357.54 & 0.82131 \\
			\hline
		\end{tabular}
		\caption{Results for different data sets}
	\end{centering}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.5\textwidth}	
		\includegraphics[width=\linewidth]{images/results/comparisonErrors.png}
		\caption{Comparison of the real error (purple) and the estimated Least Squares error (green)}
		\label{fig:comparisonErrorsLS}
	\end{subfigure}
	\caption{Results of LS method with 10 iterations based on the estimated LS error}
	\label{fig:iterationsLSwithFigure}
\end{figure}

As we can see, the results in terms of estimation error are exactly the same as the ones seen in table \ref{tb:basicCutoff} with a slight increase in execution time due to the number of iterations. Because there is no improvement with consecutive iterations, the algorithm just keeps the best result from the first one, hence the same results.

Despite the fact that using this approach does not improve the results, it can be interesting to see how the estimated LS error evolves compared to the real error for all the studied data sets, figure \ref{fig:comparisonErrorsLS} shows a comparison for all the tested data sets.

%\begin{figure}[!htb]
%	\begin{centering}
%		\includegraphics[width=0.5\linewidth]{images/results/comparisonErrors.png}
%		\caption{Comparison of the real error (purple) and the estimated Least Squares error (green)}
%		\label{fig:comparisonErrorsLS}
%	\end{centering}
%\end{figure}

As we have seen in previous sections, some of the studied flares are hard to detect for both methods, due to their intensity. We can observe that both functions have similar spikes: if the solution has a large error, it is because the Least Squares method could not provide a good solution with the available data. 

\subsection{Discarding by the residual sum of squares}

In the Least Squares chapter a slight improvement could be observed when discarding outliers based on the residual sum of squares of each iteration, but only for a specific flare. These are the results of the total error and total execution time for all 10 data sets using up to 30 iterations.

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Iterations & Error (º) & Time (s) \\ [0.5ex] 
		\hline\hline
		2 & 375.793 & 0.741631 \\
		\hline
		3 & 458.501 & 0.752343 \\
		\hline
		4 & 454.244 & 0.736867 \\
		\hline
		5 & 422.8 & 0.744048 \\
		\hline
		6 & 447.083 & 0.767054 \\
		\hline
		7 & 445.304 & 0.813639 \\
		\hline
		8 & 457.769 & 0.80455 \\
		\hline
		9 & 401.549 & 0.87425 \\
		\hline
		10 & 385.327 & 0.824476 \\
		\hline
		11 & 370.055 & 0.838879 \\
		\hline
	\end{tabular}
	\quad\quad\quad
	\begin{tabular}{|c c c|} 
		\hline
		Iterations & Error (º) & Time (s) \\ [0.3ex] 
		\hline\hline
		12 & 387.599 & 0.893248 \\
		\hline
		13 & 371.906 & 0.893959 \\
		\hline
		14 & 376.933 & 0.910624 \\
		\hline
		15 & 365.973 & 0.934418 \\
		\hline
		... & ... & ... \\
		\hline
		22 & 376.933 & 0.939168 \\
		\hline
		23 & 365.973 & 0.915974 \\
		\hline
		... & ... & ... \\
		\hline
		29 & 376.933 & 0.95647 \\
		\hline
		30 & 365.973 & 1.16539 \\
		\hline
	\end{tabular}
	\caption{Total estimation error and total execution time for different data sets using LS method with iterations}
\end{table}

As we can see, the total error increases significantly once the algorithm starts iterating but the results slowly improve (and the execution time increases) until iterations 14 and 15, where the algorithm periodically yields the same results every two iterations. Still, no iteration presents any improvement in comparison to the first one: 357.54 degrees, so using a single iteration appears to be the best option for the Least Squares method.

\clearpage

\section{Using multiple epochs}

Although this change did not come up when testing the data sets from the Sun but those from far-away stars (in the next chapter), using multiple epochs resulted in better estimations with the Sun as well so it is included in this chapter.

As we have seen when reducing the number of samples by discarding oultiers with the Least Square method, the results did not improve due to less equations being used for the system. Because of this, we decided to test the algorithm using data from more than one epoch, that is, when filtering by the time of the best found epoch, also filtering by the time of the second best one (or third), increasing the number of samples used by the algorithm. These are the results of using the two best epochs rather than just the best one, and only using a cutoff filter:

\begin{table}[h!]
	\centering
	\def\arraystretch{1.2}
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.5ex] 
		\hline\hline
		2001.347 & 6.47413 & 2.14455 \\
		\hline
		2002.196 & 23.1275 & 0.59226 \\
		\hline
		2003.301 & 1.86704 & 2.0248 \\
		\hline
		2003.308 & 94.6148 & 1.19993 \\
		\hline
		2005.020 & 72.4195 & 1.18676 \\
		\hline
		2005.258 & 44.2516 & 1.92844 \\
		\hline
		2011.210 & 11.4821 & 2.54239 \\
		\hline
		2012.066 & 89.4625 & 3.97582 \\
		\hline
		2012.130 & 20.645 & 4.37908 \\
		\hline
		2012.297 & 4.82785 & 4.78413 \\
		\hline
		Total & 369.172 & 24.7582\\
		\hline
	\end{tabular}
	\quad\quad\quad
	\begin{tabular}{|c c c|} 
		\hline
		Data set & Error (º) & Time (s) \\ [0.5ex] 
		\hline\hline
		2001.347 & 4.49127 & 0.161254 \\
		\hline
		2002.196 & 8.30599 & 0.00555821 \\
		\hline
		2003.301 & 4.25894 & 0.00930464 \\
		\hline
		2003.308 & 129.135 & 0.363268 \\
		\hline
		2005.020 & 40.0696 & 0.13032 \\
		\hline
		2005.258 & 12.3223 & 0.011427 \\
		\hline
		2011.210 & 6.11407 & 0.0623802 \\
		\hline
		2012.066 & 43.7512 & 0.0282298 \\
		\hline
		2012.130 & 7.72546 & 0.0567187 \\
		\hline
		2012.297 & 3.8804 & 0.0432351 \\
		\hline
		Total & 260.055 & 0.871695\\
		\hline
	\end{tabular}
	\caption{Estimation error and execution time for different data sets using the DR (left) and LS (right) methods with a cutoff filter and using data from two epochs}
\end{table}

Although using linear fit to discard outliers with the DR method seemed to improve the results in previous sections, using it with multiple epochs does not present any improvement (the total error of the DR with linear fit for two epochs is 400.146). But it is still an improvement in comparison to the DR method using only the cutoff filter. Regarding the LS method, using multiple iterations with two epochs reduces the error, but not iterating at all is again the combination that yields the lowest error values, a total of 260.055 degrees. Although some of the flares are detected with a slightly higher error, the total is significantly lower.

Furthermore, using 3 epochs instead of 2 resulted in a total error of 390.954 for the DR method and 294.689 for the LS method, and there is no improvement beyond using 3 epochs. This change was very important for some tests presented in the next chapter.

\clearpage

\section{Discussion}

In this chapter we have seen that working with the entire data set of IPPs does not yield good results, there is too much noise and the results differ considerably from the real position of the Sun. Considering that this is a new method to detect flares, however, this could have been the case in general (the detection could not have been possible, or at least not with enough precision), but when using a direct filter for the VTEC values we have seen that some of the data sets yield results with errors as small as 4 degrees.

It can be interesting to see, however, how some data sets still have a high error, due to the nature of the flare (perhaps it was not sufficiently strong to have an impact on the ionosphere), figure \ref{fig:comparisonDRLS}a compares the Least Squares and Decreasing Range methods' error of the direct filter approach.

\begin{figure}[!htb]
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\linewidth]{images/results/comparisonErrorsDRLS.png}
		\caption{Estimation error}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\linewidth]{images/results/comparisonTimeDRLS.png}
		\caption{Execution time}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\linewidth]{images/results/errorVSmeanVTEC.png}
		\caption{Mean VTEC as a functiondsadasr}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.5\textwidth}
		\includegraphics[width=\linewidth]{images/results/meanVTECVSerror.png}
		\caption{dsadasdMean VTEC as a functiondsadas}
	\end{subfigure}
	\caption{Comparison of the estimation error and execution time for the Least Squares and Decreasing Range methods}
	\label{fig:comparisonDRLS}
\end{figure}

Furthermore, the correlation between the intensity of the flare and the error of the estimation can be seen in figure \ref{fig:comparisonDRLS}c if we plot the mean VTEC of the best epoch that the algorithm uses (representative of the strength of the flare) as a function of the estimation error. Or the inverse (figure \ref{fig:comparisonDRLS}d. These two plots are representative of how the estimation error increases as the power of the flare decrease.

Regarding the performance of the algorithm, figure \ref{fig:comparisonDRLS}b compares the execution time of both the LS (green) and DR (purple) methods. The DR method takes more time for all data sets and there would be an even greater difference would we compare the method using linear fit.

In conclusion, the Decreasing Range method (using linear fit to discard outliers) and the Least Squares method (using a direct filter) provide good results for some of the data sets with powerful flares, therefore, these two methods will. However, the combination that yields the best results is the Least Squares method using a direct filter and data from the two best epochs rather than just one.

Therefore, this will be the main method used to study stellar flares in the next chapter, although the other presented options will be taken into consideration as well.

